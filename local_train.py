# -*- coding: utf-8 -*-
"""lstm_review_pytorch.py - Local Windows PC Version"""

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
# kobert-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì§ì ‘ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from transformers import BertModel, BertTokenizer # <-- BERT í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©
from transformers import AutoModel # <-- AutoModelì€ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥

import pandas as pd
import numpy as np
import urllib.request
import os # ê²½ë¡œ ì²˜ë¦¬ë¥¼ ìœ„í•´ os ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.

# =================================================================
# 1. ë¡œì»¬ í™˜ê²½ ì„¤ì • ë° ê²½ë¡œ ì§€ì •
# =================================================================

# í›ˆë ¨ëœ ëª¨ë¸ì„ ì €ì¥í•  ë¡œì»¬ í´ë” ê²½ë¡œë¥¼ ë³€ìˆ˜ë¡œ ì§€ì •í•©ë‹ˆë‹¤.
# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ì‹¤í–‰ë˜ëŠ” ë””ë ‰í„°ë¦¬ ë‚´ì— 'my_local_models/lstm/review_classify' í´ë”ê°€ ìƒì„±ë©ë‹ˆë‹¤.
# Windowsì—ì„œëŠ” ìƒëŒ€ ê²½ë¡œ('./')ë¥¼ ì‚¬ìš©í•˜ë©´ í¸ë¦¬í•©ë‹ˆë‹¤.
LOCAL_PATH = './my_local_models/lstm/review_classify'

# í´ë”ê°€ ì—†ë‹¤ë©´ ìƒì„±í•©ë‹ˆë‹¤.
os.makedirs(LOCAL_PATH, exist_ok=True)
print(f"ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {os.path.abspath(LOCAL_PATH)}")

# GPU ì‚¬ìš© ì„¤ì • (KoBERT ëª¨ë¸ ë¡œë“œ ë° device ì„¤ì •ì€ ì´ì „ ì½”ë“œ ê·¸ëŒ€ë¡œ ìœ ì§€)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ì‚¬ìš© ì¥ì¹˜: {device}")
#device="cpu"

# =================================================================
# 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# =================================================================

# ë°ì´í„° ë¡œë“œ (ë„¤ì´ë²„ ì‡¼í•‘ ë¦¬ë·° ë°ì´í„° ìë™ ë‹¤ìš´ë¡œë“œ)
DATA_FILE = 'shopping.txt'
print(f"ë°ì´í„° {DATA_FILE} ë‹¤ìš´ë¡œë“œ ì¤‘...")
"""
urllib.request.urlretrieve(
    'https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt',
    DATA_FILE
)
"""
reg_txt = "[^ã„±-ã…ã…-ã…£ê°€-í£a-zA-Z\s]"
raw = pd.read_table(DATA_FILE, names=['rating','review'])
raw = raw[:6000] 
raw['label'] = np.where(raw['rating']>3, 1, 0)
print(f"ì´ {len(raw)}ê°œì˜ ë¦¬ë·° ë°ì´í„° ë¡œë“œ ì™„ë£Œ.")

# ë¦¬ë·° ë°ì´í„°ì˜ NaN ê°’(ê²°ì¸¡ì¹˜)ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
raw['review'] = raw['review'].fillna('')

# ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë¦¬ë·°ì—ì„œ ë¶ˆí•„ìš”í•œ ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤.
raw['review'] = raw['review'].str.replace(reg_txt, '', regex=True)

# ê¸¸ì´ê°€ 0ì¸ ë¹ˆ ë¬¸ìì—´ ë¦¬ë·°ë¥¼ ì œê±° (ê°€ì¥ ì¤‘ìš”í•œ ìˆ˜ì • ë¶€ë¶„)
# ë¹ˆ ë¬¸ìì—´('')ì´ ì•„ë‹Œ ë¦¬ë·°ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
raw = raw[raw['review'].str.strip() != '']
# str.strip()ì„ ì‚¬ìš©í•˜ì—¬ ê³µë°±ë§Œ ë‚¨ì€ ë¦¬ë·°ë„ ì œê±°ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

print(f"ì •ê·œí‘œí˜„ì‹ ì ìš© í›„, ë¹ˆ ë¦¬ë·°ë¥¼ ì œê±°í•˜ì—¬ ì´ {len(raw)}ê°œì˜ ë¦¬ë·° ë°ì´í„°ê°€ ë‚¨ì•˜ìŠµë‹ˆë‹¤.")

# í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
MODEL_NAME = 'monologg/kobert' # <-- ëª¨ë¸ ì´ë¦„ì„ ë” ì•ˆì •ì ì¸ monologg ë²„ì „ìœ¼ë¡œ ë³€ê²½
# 1. BertTokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME) # <-- BertTokenizer ì‚¬ìš©

# 2. BertModelì„ ì‚¬ìš©í•˜ì—¬ ë¡œë“œ
kobert_model = BertModel.from_pretrained(MODEL_NAME).to(device)#.eval() # <-- AutoModel ëŒ€ì‹  BertModel ì‚¬ìš©

EMBEDDING_DIM = kobert_model.config.hidden_size # 768

# ì „ì²´ ë°ì´í„°ì…‹ ì¸ì½”ë”©
reviews = raw['review'].tolist()

print("ì „ì²´ ë¦¬ë·° ë°ì´í„° ì¸ì½”ë”© ì¤‘...")
tokenized_data = tokenizer(
    reviews,
    padding='max_length',
    truncation=True,
    max_length=256,
    return_tensors='pt'
)
labels = torch.tensor(raw['label'].values)
tokenized_data['labels'] = labels
print("ì¸ì½”ë”© ì™„ë£Œ.")


# --- 3. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ ë° DataLoader ì¤€ë¹„ ---

from sklearn.model_selection import train_test_split
# ë°ì´í„° ë¶„í•  (ì˜ˆ: 80% í•™ìŠµ, 20% ê²€ì¦)
input_ids = tokenized_data['input_ids']
attention_mask = tokenized_data['attention_mask']

train_indices, val_indices = train_test_split(
    range(len(labels)),
    test_size=0.2, # ê²€ì¦ ì…‹ ë¹„ìœ¨ 20%
    stratify=labels.cpu().numpy(),
    random_state=42
)

# í•™ìŠµ ë°ì´í„°ì…‹ ì¤€ë¹„
train_dataset = TensorDataset(
    input_ids[train_indices],
    attention_mask[train_indices],
    labels[train_indices]
)

# ê²€ì¦ ë°ì´í„°ì…‹ ì¤€ë¹„
val_dataset = TensorDataset(
    input_ids[val_indices],
    attention_mask[val_indices],
    labels[val_indices]
)

BATCH_SIZE = 32 # ë°°ì¹˜ í¬ê¸° ì„¤ì •

# DataLoader ìƒì„±
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)


# =================================================================
# 4. GRU ê¸°ë°˜ ë¶„ë¥˜ê¸° ëª¨ë¸ ì •ì˜
# =================================================================

class GRUClassifier(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, output_dim, num_layers, dropout):
        super(GRUClassifier, self).__init__()
        self.rnn = nn.GRU(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            bidirectional=True,
            dropout=dropout,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, text, attention_mask):
        global kobert_model # ì „ì—­ ë³€ìˆ˜ KoBERT ëª¨ë¸ ì‚¬ìš©

        # 1. KoBERT ì„ë² ë”© ì¶”ì¶œ 
        with torch.no_grad():
            outputs = kobert_model(input_ids=text, attention_mask=attention_mask)
            embedded = outputs.last_hidden_state

        # 2. GRU ë ˆì´ì–´ í†µê³¼
        rnn_output, hidden = self.rnn(embedded)

        # 3. ìµœì¢… ì€ë‹‰ ìƒíƒœë¥¼ ë¶„ë¥˜ê¸°ì— ì‚¬ìš© (ìˆœë°©í–¥/ì—­ë°©í–¥ ê²°í•©)
        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))

        # 4. ì„ í˜• ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œì¼œ ìµœì¢… ì˜ˆì¸¡ê°’ ê³„ì‚°
        prediction = self.fc(hidden)

        return prediction


# =================================================================
# 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜
# =================================================================

from sklearn.metrics import accuracy_score

def train(model, dataloader, optimizer, criterion, device):
    model.train()
    epoch_loss = 0

    for batch in dataloader:
        input_ids, attention_mask, labels = [t.to(device) for t in batch]

        optimizer.zero_grad()
        predictions = model(input_ids, attention_mask)
        loss = criterion(predictions, labels)

        loss.backward()
        optimizer.step()

        epoch_loss += loss.item() * len(labels)

    return epoch_loss / len(dataloader.dataset)

def evaluate(model, dataloader, criterion, device):
    model.eval() # í‰ê°€ ëª¨ë“œ (Dropout, BatchNorm ë¹„í™œì„±í™”)
    epoch_loss = 0
    all_predictions = []
    all_labels = []

    with torch.no_grad(): # í‰ê°€ ì‹œì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¶ˆí•„ìš”
        for batch in dataloader:
            input_ids, attention_mask, labels = [t.to(device) for t in batch]

            predictions = model(input_ids, attention_mask)
            loss = criterion(predictions, labels)

            epoch_loss += loss.item() * len(labels)

            all_predictions.extend(predictions.argmax(1).cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # ì •í™•ë„ ê³„ì‚°
    accuracy = accuracy_score(all_labels, all_predictions)

    return epoch_loss / len(dataloader.dataset), accuracy


# =================================================================
# 6. í•™ìŠµ ì‹¤í–‰
# =================================================================

HIDDEN_DIM = 512 
OUTPUT_DIM = 2
NUM_LAYERS = 2
DROPOUT = 0.5
LEARNING_RATE_GRU  = 1e-4 # 1e-4 (0.0001), 5e-4 (0.0005) 
N_EPOCHS = 20

model = GRUClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS, DROPOUT).to(device)
criterion = nn.CrossEntropyLoss()

# ğŸ’¡ ì—¬ê¸°ì— ìˆ˜ì •ëœ KoBERT Fine-Tuning ì½”ë“œë¥¼ ì‚½ì…í•©ë‹ˆë‹¤. ğŸ’¡
# ------------------------------------------------------------------
# KoBERT Fine-Tuningì„ ìœ„í•œ ì„¤ì •
# ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì • (ê¸°ì¡´ì— Freezeí–ˆë˜ ìƒíƒœë¥¼ ë‹¤ì‹œ ì‹œì‘)
for param in kobert_model.parameters():
    param.requires_grad = False

# KoBERTì˜ ë§ˆì§€ë§‰ íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ í—ˆìš© (Unfreeze)
# KoBERTëŠ” ë³´í†µ 12ê°œì˜ ë ˆì´ì–´(0~11)ë¥¼ ê°€ì§€ë¯€ë¡œ [-i]ì€ ë§ˆì§€ë§‰ ë ˆì´ì–´ì…ë‹ˆë‹¤.
"""
for i in range(i): # ë§ˆì§€ë§‰ 2ê°œ ë ˆì´ì–´
    for param in kobert_model.encoder.layer[-(i+1)].parameters(): 
        param.requires_grad = True
"""

# 3. KoBERT Fine-Tuningì— ì í•©í•œ ë‚®ì€ í•™ìŠµë¥  ì„¤ì •
LEARNING_RATE_FT = 2e-5 # 0.00002

#KoBERT ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë„ ì˜µí‹°ë§ˆì´ì €ì— í¬í•¨
all_params = list(model.parameters()) + list(kobert_model.parameters()) 
optimizer = torch.optim.Adam([
    # GRU ë¶„ë¥˜ê¸° íŒŒë¼ë¯¸í„°: ë¹ ë¥¸ í•™ìŠµë¥  ì ìš©
    {'params': model.parameters(), 'lr': LEARNING_RATE_GRU}, 
    # KoBERT íŒŒë¼ë¯¸í„° (requires_grad=Trueì¸ ê²ƒë“¤ë§Œ í•™ìŠµë¨): ë‚®ì€ í•™ìŠµë¥  ì ìš©
    {'params': kobert_model.parameters(), 'lr': LEARNING_RATE_FT}
]) 


print("\n" + "=" * 60)
print(f"GRU ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (í•™ìŠµ: {len(train_dataset)}ê°œ, ê²€ì¦: {len(val_dataset)}ê°œ)")
print("=" * 60)

best_val_loss = float('inf')

for epoch in range(N_EPOCHS):
    # í•™ìŠµ
    train_loss = train(model, train_dataloader, optimizer, criterion, device)

    # ê²€ì¦
    val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)

    # ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥ (ì†ì‹¤ì´ ê°€ì¥ ë‚®ì„ ë•Œ)
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        # ì €ì¥ ê²½ë¡œ ìˆ˜ì • (ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©)
        SAVE_PATH = os.path.join(LOCAL_PATH, 'gru_classifier_best.pt')
        torch.save(model.state_dict(), SAVE_PATH)

    print(f'ì—í­: {epoch+1:02} | í•™ìŠµ ì†ì‹¤: {train_loss:.4f} | ê²€ì¦ ì†ì‹¤: {val_loss:.4f} | ê²€ì¦ ì •í™•ë„: {val_acc:.4f}')

print("=" * 60)
print("í•™ìŠµ ì™„ë£Œ.")
print(f"ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì´ '{SAVE_PATH}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ìµœì†Œ ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f})")